{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Graphs and TF Functions\n",
        "\n",
        "Resource: [TensorFlow Guide](https://www.tensorflow.org/guide/intro_to_graphs)"
      ],
      "metadata": {
        "id": "DjXe2n1ztin4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In TensorFlow, eager execution is a mode that allows you to run TensorFlow operations immediately as they are called from Python. This is in contrast to graph execution where tensor computations are executed as a TensorFlow graph, sometimes referred to as a tf.Graph or simply a “graph.”\n",
        "\n",
        "Graph execution enables portability outside Python and tends to offer better performance. Eager execution simplifies the model building experience in TensorFlow, whereas graph execution can provide optimizations that make models run faster with better memory efficiency.\n",
        "\n",
        "Graphs are data structures that contain a set of tf.Operation objects, which represent units of computation; and tf.Tensor objects, which represent the units of data that flow between operations. They are defined in a tf.Graph context. Since these graphs are data structures, they can be saved, run, and restored all without the original Python code."
      ],
      "metadata": {
        "id": "ZOgcS2q4uEAk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2yxTpR1Hq21f"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import timeit\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Graphs\n",
        "### Creating a graph with tf.function"
      ],
      "metadata": {
        "id": "iRaH9pMBvwE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regular_function(x, y, b):\n",
        "  x = tf.matmul(x, y)\n",
        "  x = x + b\n",
        "  return b\n",
        "\n",
        "tf_function = tf.function(regular_function)"
      ],
      "metadata": {
        "id": "ra9tB-91vt-d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(regular_function))\n",
        "print(type(tf_function))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN1FdETrw7Gu",
        "outputId": "f9502fd9-dfff-4684-f8cc-f2b92ea184aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'function'>\n",
            "<class 'tensorflow.python.eager.polymorphic_function.polymorphic_function.Function'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = tf.constant([[1.0, 2.0]])\n",
        "y1 = tf.constant([[2.0], [3.0]])\n",
        "b1 = tf.constant(4.0)"
      ],
      "metadata": {
        "id": "BkjfLO9Gw-uS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res1 = regular_function(x1, y1, b1).numpy()\n",
        "res2 = tf_function(x1, y1, b1).numpy()\n",
        "\n",
        "assert res1 == res2\n",
        "\n",
        "print(res1)\n",
        "print(res2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfS3qolqxSOj",
        "outputId": "52afe3f3-dfe5-44de-a561-fde7d8d1224c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0\n",
            "4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# graph-generating output of AutoGraph.\n",
        "print(tf.autograph.to_code(regular_function))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbFdfnUkyyUq",
        "outputId": "f92223c6-a056-44bf-9310-8b85b952f833"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def tf__regular_function(x, y, b):\n",
            "    with ag__.FunctionScope('regular_function', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
            "        do_return = False\n",
            "        retval_ = ag__.UndefinedReturnValue()\n",
            "        x = ag__.converted_call(ag__.ld(tf).matmul, (ag__.ld(x), ag__.ld(y)), None, fscope)\n",
            "        x = ag__.ld(x) + ag__.ld(b)\n",
            "        try:\n",
            "            do_return = True\n",
            "            retval_ = ag__.ld(b)\n",
            "        except:\n",
            "            do_return = False\n",
            "            raise\n",
            "        return fscope.ret(retval_, do_return)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting Python functions to graphs\n",
        "\n",
        "While TensorFlow operations are easily captured by a tf.Graph, Python-specific logic (e.g, if-then clauses, loops, break, return, continue, and more) needs to undergo an extra step in order to become part of the graph. tf.function uses a library called AutoGraph (tf.autograph) to convert Python code into graph-generating code."
      ],
      "metadata": {
        "id": "CUP-euxhxqGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def tf_simple_relu(x):\n",
        "  if tf.greater(x, 0):\n",
        "    return x\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "print(\"First branch, with graph:\", tf_simple_relu(tf.constant(1)).numpy())\n",
        "print(\"Second branch, with graph:\", tf_simple_relu(tf.constant(-1)).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp2lZz0NxVDU",
        "outputId": "a8ed712b-9d8d-433f-ee1f-9ec271b62945"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First branch, with graph: 1\n",
            "Second branch, with graph: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_relu(x):\n",
        "  if tf.greater(x, 0):\n",
        "    return x\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "# graph-generating output of AutoGraph.\n",
        "print(tf.autograph.to_code(simple_relu))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8Heg9hdy1eH",
        "outputId": "59412882-14a9-451f-f716-51ce6f1a8366"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def tf__simple_relu(x):\n",
            "    with ag__.FunctionScope('simple_relu', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:\n",
            "        do_return = False\n",
            "        retval_ = ag__.UndefinedReturnValue()\n",
            "\n",
            "        def get_state():\n",
            "            return (do_return, retval_)\n",
            "\n",
            "        def set_state(vars_):\n",
            "            nonlocal retval_, do_return\n",
            "            (do_return, retval_) = vars_\n",
            "\n",
            "        def if_body():\n",
            "            nonlocal retval_, do_return\n",
            "            try:\n",
            "                do_return = True\n",
            "                retval_ = ag__.ld(x)\n",
            "            except:\n",
            "                do_return = False\n",
            "                raise\n",
            "\n",
            "        def else_body():\n",
            "            nonlocal retval_, do_return\n",
            "            try:\n",
            "                do_return = True\n",
            "                retval_ = 0\n",
            "            except:\n",
            "                do_return = False\n",
            "                raise\n",
            "        ag__.if_stmt(ag__.converted_call(ag__.ld(tf).greater, (ag__.ld(x), 0), None, fscope), if_body, else_body, get_state, set_state, ('do_return', 'retval_'), 2)\n",
            "        return fscope.ret(retval_, do_return)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Polymorphism: one Function, many graphs\n",
        "\n",
        "A tf.Graph is specialized to a specific type of inputs (for example, tensors with a specific dtype or objects with the same id()).\n",
        "\n",
        "Each time you invoke a Function with a set of arguments that can't be handled by any of its existing graphs (such as arguments with new dtypes or incompatible shapes), Function creates a new tf.Graph specialized to those new arguments. The type specification of a tf.Graph's inputs is known as its input signature or just a signature."
      ],
      "metadata": {
        "id": "0gJg5PQK0_gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def my_relu(x):\n",
        "  return tf.maximum(0., x)\n",
        "\n",
        "print(my_relu(tf.constant(5.5)))\n",
        "print(my_relu([1, -2]))\n",
        "print(my_relu(tf.constant([3., -3.])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSzoAk04zj4_",
        "outputId": "3648fb89-f4a0-41fb-f341-51ac7bf4064e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(5.5, shape=(), dtype=float32)\n",
            "tf.Tensor([1. 0.], shape=(2,), dtype=float32)\n",
            "tf.Tensor([3. 0.], shape=(2,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because it's backed by multiple graphs, a Function is polymorphic. That enables it to support more input types than a single tf.Graph could represent, and to optimize each tf.Graph for better performance."
      ],
      "metadata": {
        "id": "yGSaFDan2Frd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# There are three `ConcreteFunction`s (one for each graph) in `my_relu`.\n",
        "# The `ConcreteFunction` also knows the return type and shape!\n",
        "print(my_relu.pretty_printed_concrete_signatures())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqgI6pCA1guG",
        "outputId": "80881d4a-418e-4a8d-e833-4c61cc9f25fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_relu(x)\n",
            "  Args:\n",
            "    x: float32 Tensor, shape=()\n",
            "  Returns:\n",
            "    float32 Tensor, shape=()\n",
            "\n",
            "my_relu(x=[1, -2])\n",
            "  Returns:\n",
            "    float32 Tensor, shape=(2,)\n",
            "\n",
            "my_relu(x)\n",
            "  Args:\n",
            "    x: float32 Tensor, shape=(2,)\n",
            "  Returns:\n",
            "    float32 Tensor, shape=(2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_relu(tf.constant([-3., 3., -3.])))\n",
        "print(my_relu(tf.Variable([[1., 2.], [4., 2.]])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srs0r8612Pqu",
        "outputId": "a593b9d9-5643-4f94-8e6f-6b402d992211"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function my_relu at 0x7fbf0cadf010> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 3. 0.], shape=(3,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1. 2.]\n",
            " [4. 2.]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(my_relu.pretty_printed_concrete_signatures())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mddoj7Kv2RuN",
        "outputId": "b0a62f48-fbd8-43d1-c4ef-295fbe7c6512"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my_relu(x)\n",
            "  Args:\n",
            "    x: float32 Tensor, shape=()\n",
            "  Returns:\n",
            "    float32 Tensor, shape=()\n",
            "\n",
            "my_relu(x=[1, -2])\n",
            "  Returns:\n",
            "    float32 Tensor, shape=(2,)\n",
            "\n",
            "my_relu(x)\n",
            "  Args:\n",
            "    x: float32 Tensor, shape=(2,)\n",
            "  Returns:\n",
            "    float32 Tensor, shape=(2,)\n",
            "\n",
            "my_relu(x)\n",
            "  Args:\n",
            "    x: float32 Tensor, shape=(3,)\n",
            "  Returns:\n",
            "    float32 Tensor, shape=(3,)\n",
            "\n",
            "my_relu(x)\n",
            "  Args:\n",
            "    x: VariableSpec(shape=(2, 2), dtype=tf.float32, trainable=True, alias_id=0)\n",
            "  Returns:\n",
            "    float32 Tensor, shape=(2, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using tf.function Correctly\n",
        "### Graph vs eager executions\n",
        "The code in a Function can be executed both eagerly and as a graph. By default, Function executes its code as a graph."
      ],
      "metadata": {
        "id": "bUkiu4nf2uG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def get_MSE(y_true, y_pred):\n",
        "  sq_diff = tf.pow(y_true - y_pred, 2)\n",
        "  return tf.reduce_mean(sq_diff)"
      ],
      "metadata": {
        "id": "NlKBCWuw2TGx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
        "y_pred = tf.random.uniform([5], maxval=10, dtype=tf.int32)\n",
        "\n",
        "print(get_MSE(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKDm0W_h4Mlj",
        "outputId": "d40262ca-172f-4e36-d7b5-a69843616d62"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(7, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# running the function eagerly\n",
        "tf.config.run_functions_eagerly(True)\n",
        "print(get_MSE(y_true, y_pred))\n",
        "\n",
        "tf.config.run_functions_eagerly(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci88TTHZ4N9B",
        "outputId": "13780b28-e778-49ea-9a2d-2646cc132d4e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(7, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function can behave differently under graph and eager execution. The Python print function is one example of how these two modes differ. Let's check out what happens when you insert a print statement to your function and call it repeatedly."
      ],
      "metadata": {
        "id": "OGWWsOrx4mXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def get_MSE(y_true, y_pred):\n",
        "  print(\"Calculating MSE!\")\n",
        "  sq_diff = tf.pow(y_true - y_pred, 2)\n",
        "  return tf.reduce_mean(sq_diff)"
      ],
      "metadata": {
        "id": "Th-1el-G4Zf1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error1 = get_MSE(y_true, y_pred)\n",
        "error2 = get_MSE(y_true, y_pred)\n",
        "error3 = get_MSE(y_true, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jAFY6sk5P2G",
        "outputId": "b4056d38-1902-4a5d-88ba-d16565b332d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating MSE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(error1)\n",
        "print(error2)\n",
        "print(error3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nf6DvHzR5Wdy",
        "outputId": "1288486c-47ec-45bc-f969-18f3a8131064"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n",
            "tf.Tensor(7, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "get_MSE only printed once even though it was called three times.\n",
        "\n",
        "The print statement is executed when Function runs the original code in order to create the graph in a process known as \"tracing\"\n",
        "\n",
        "Tracing captures the TensorFlow operations into a graph, and print is not captured in the graph. That graph is then executed for all three calls without ever running the Python code again."
      ],
      "metadata": {
        "id": "HSUKSv9K5NSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-strict Execution\n"
      ],
      "metadata": {
        "id": "BA1e2-G95n11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Eager execution steps through all program operations even when it is not needed"
      ],
      "metadata": {
        "id": "sivisJSfYLzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # this raises an error\n",
        "  tf.gather(tf.constant([0.0]), [1])\n",
        "except tf.errors.InvalidArgumentError as e:\n",
        "  # All operations are run during eager execution so an error is raised.\n",
        "  print(f'{type(e).__name__}: {e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIBf6CVZXoQo",
        "outputId": "67f772f9-e3f4-4830-e747-b250a468c3c8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InvalidArgumentError: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0] = 1 is not in [0, 1) [Op:GatherV2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unused_return_eager(x):\n",
        "  # Get index 1 will fail when `len(x) == 1`\n",
        "  tf.gather(x, [1]) # unused\n",
        "  return x\n",
        "\n",
        "try:\n",
        "  print(unused_return_eager(tf.constant([0.0])))\n",
        "except tf.errors.InvalidArgumentError as e:\n",
        "  # All operations are run during eager execution so an error is raised.\n",
        "  print(f'{type(e).__name__}: {e}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPnxdCSh5Qyr",
        "outputId": "6753e49a-620b-4cb4-ba89-bb2e0f5c54f4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "InvalidArgumentError: {{function_node __wrapped__GatherV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[0] = 1 is not in [0, 1) [Op:GatherV2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph execution ignores unnecessary operations"
      ],
      "metadata": {
        "id": "jgxP0FkqX5L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def unused_return_graph(x):\n",
        "  tf.gather(x, [1]) # unused\n",
        "  return x\n",
        "\n",
        "# Only needed operations are run during graph execution. The error is not raised.\n",
        "print(unused_return_graph(tf.constant([0.0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HnL5vxEXJ82",
        "outputId": "31b697d4-f95f-442b-d1de-47fb7ce81ae4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unused_return_graph(x):\n",
        "  tf.print(\"this is the unused return graph function\")\n",
        "  tf.gather(x, [1]) # unused\n",
        "  return x\n",
        "\n",
        "tf_unused_return_graph = tf.function(unused_return_graph)\n",
        "\n",
        "# Only needed operations are run during graph execution. The error is not raised.\n",
        "print(tf_unused_return_graph(tf.constant([0.0])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gSfBnbUXOEf",
        "outputId": "f174418f-6046-4e36-f3c7-53fdf8efecbf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this is the unused return graph function\n",
            "tf.Tensor([0.], shape=(1,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tf.function Best Practices\n",
        "\n",
        "Designing for tf.function may be your best bet for writing graph-compatible TensorFlow programs. Here are some tips:\n",
        "* Toggle between eager and graph execution early and often with tf.config.run_functions_eagerly to pinpoint if/ when the two modes diverge.\n",
        "* Create tf.Variables outside the Python function and modify them on the inside. The same goes for objects that use tf.Variable, like tf.keras.layers, tf.keras.Models and tf.keras.optimizers.\n",
        "* Avoid writing functions that depend on outer Python variables, excluding tf.Variables and Keras objects. Learn more in Depending on Python global and free variables of the tf.function guide.\n",
        "* Prefer to write functions which take tensors and other TensorFlow types as input. You can pass in other object types but be careful! Learn more in Depending on Python objects of the tf.function guide.\n",
        "* Include as much computation as possible under a tf.function to maximize the performance gain. For example, decorate a whole training step or the entire training loop."
      ],
      "metadata": {
        "id": "1_RXzu9kYg-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speeding up\n",
        "\n",
        "tf.function usually improves the performance of your code, but the amount of speed-up depends on the kind of computation you run. Small computations can be dominated by the overhead of calling a graph.\n",
        "\n",
        "tf.function is commonly used to speed up training loops.\n",
        "\n",
        "You can also try tf.function(jit_compile=True) for a more significant performance boost, especially if your code is heavy on TensorFlow control flow and uses many small tensors."
      ],
      "metadata": {
        "id": "YxZ-LHuWZANb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def power(x, y):\n",
        "  result = tf.eye(10, dtype=tf.dtypes.int32)\n",
        "  for _ in range(y):\n",
        "    result = tf.matmul(x, result)\n",
        "  return result"
      ],
      "metadata": {
        "id": "fPuGPw0jZTiL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.random.uniform(shape=[10, 10], minval=-1, maxval=2, dtype=tf.dtypes.int32)"
      ],
      "metadata": {
        "id": "BSj-_Y3nXkhv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execution_time_eager = timeit.timeit(lambda: power(x, 100), number=1000)\n",
        "print(\"Eager execution:\",\n",
        "      execution_time_eager,\n",
        "      \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rSQYyfOZZGL",
        "outputId": "3849a3da-5999-4c45-ede2-70e596fbbf32"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eager execution: 14.120101007000017 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_power = tf.function(power)"
      ],
      "metadata": {
        "id": "Ev3GLMEdZaSD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "execution_time_graph = timeit.timeit(lambda: tf_power(x, 100), number=1000)\n",
        "print(\"Graph execution:\",\n",
        "      execution_time_graph,\n",
        "      \"seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHBDiavXZi1m",
        "outputId": "b7f7d3ec-8893-429e-e33b-66d27744f04b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph execution: 1.0472153770000432 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "execution_time_eager / execution_time_graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrV-7cIvZlH6",
        "outputId": "6439ada8-f5e7-4632-d34b-670a9b6c52f5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.48347371239893"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tracing\n",
        "\n",
        "Graphs can speed up your code, but the process of creating them has some overhead. For some functions, the creation of the graph takes more time than the execution of the graph. This investment is usually quickly paid back with the performance boost of subsequent executions, but it's important to be aware that the first few steps of any large model training can be slower due to tracing.\n",
        "\n",
        "No matter how large your model, you want to avoid tracing frequently. The tf.function guide discusses how to set input specifications and use tensor arguments to avoid retracing in the Controlling retracing section. If you find you are getting unusually poor performance, it's a good idea to check if you are retracing accidentally.\n",
        "\n",
        "To figure out when your Function is tracing, add a print statement to its code. As a rule of thumb, Function will execute the print statement every time it traces."
      ],
      "metadata": {
        "id": "Bl5Opgrbfrn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def a_function_with_python_side_effect(x):\n",
        "  print(\"Tracing!\") # An eager-only side effect.\n",
        "  return x * x + tf.constant(2)\n",
        "\n",
        "# This is traced the first time.\n",
        "print(a_function_with_python_side_effect(tf.constant(2)))\n",
        "# The second time through, you won't see the side effect.\n",
        "print(a_function_with_python_side_effect(tf.constant(3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeLc9IbJZ3mP",
        "outputId": "f36e3956-3e37-4568-cd8b-869490d4db43"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing!\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "tf.Tensor(11, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This retraces each time the Python argument changes,\n",
        "# as a Python argument could be an epoch count or other\n",
        "# hyperparameter.\n",
        "print(a_function_with_python_side_effect(2))\n",
        "print(a_function_with_python_side_effect(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKTxw9argKKY",
        "outputId": "062de69e-fc5a-4fd7-bd52-8ff055b0651b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing!\n",
            "tf.Tensor(6, shape=(), dtype=int32)\n",
            "Tracing!\n",
            "tf.Tensor(11, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a_function_with_python_side_effect(tf.constant([3])))\n",
        "print(a_function_with_python_side_effect(tf.constant([3, 3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT7HQKmVgguW",
        "outputId": "2ccce368-3c8e-4b3b-f508-dbc8f20e1d82"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 6 calls to <function a_function_with_python_side_effect at 0x7fbf0c8c9a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tracing!\n",
            "tf.Tensor([11], shape=(1,), dtype=int32)\n",
            "Tracing!\n",
            "tf.Tensor([11 11], shape=(2,), dtype=int32)\n"
          ]
        }
      ]
    }
  ]
}